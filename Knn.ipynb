{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN\n",
    "\n",
    "Is one of the most used tool, widely supported and full of libraries. It makes no conjecture about the fields of the data or how they should be cartegorized, is context agnostic.\n",
    "\n",
    "As often happens in AI, practice is far ahead compared to theory:\n",
    "- No scieintific or proof of works.\n",
    "- Just empirical evidence\n",
    "\n",
    "\n",
    "In this notebook we will implement KNN step by step, and skip the definition of K for lack of time.\n",
    "\n",
    "![kNN mechanics](images/knn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN does not assume anything about the data, other than a distance measure can be calculated consistently between any two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form.\n",
    "\n",
    "## instance-based \n",
    "\n",
    "Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model.\n",
    "\n",
    "\n",
    "## competitive learning\n",
    "\n",
    "It is a competitive learning algorithm, because it internally uses competition between model elements (data instances) in order to make a predictive decision. The objective similarity measure between data instances causes each data instance to compete to “win” or be most similar to a given unseen data instance and contribute to a prediction.\n",
    "\n",
    "\n",
    "## lazy learning algorithms\n",
    "\n",
    "Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Algorithm\n",
    "\n",
    "The kNN task can be broken down into writing 3 primary functions: \n",
    "\n",
    "    1. Calculate the distance between any two points \n",
    "    2. Find the nearest neighbours based on these pairwise distances\n",
    "    3. Majority vote on a class labels based on the nearest neighbour list \n",
    "\n",
    "![kNN Alogorithm](images/knn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "This should always be the first step when tackling a Machine Learning problem. We must clean, sanitize and organize the data.\n",
    "In this problem we will skip this part, as usually does not require any domain specific knowledge, but just a lot of patience\n",
    "\n",
    "Once the data clean we can split to get ready for the three phases\n",
    "- Train 60%\n",
    "- Test 20%\n",
    "- Validate 20%\n",
    "\n",
    "this can be a good ratio to split your data, also 80, 10, 10 will work fine. Do not underestimate the importance of test and validation set, as they are key points to avoid overfitting and to check the accuracy of your models.\n",
    "\n",
    "In our case we will need only Train and Test set, as we are working on a simplified problem.\n",
    "\n",
    "![data preparation](images/knn3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karoly.szabo\\AppData\\Local\\Continuum\\anaconda3\\envs\\dog-project\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    " \n",
    "# 1) given two data points, calculate the euclidean distance between them\n",
    "def get_distance(data1, data2):\n",
    "    points = zip(data1, data2)\n",
    "    diffs_squared_distance = [pow(a - b, 2) for (a, b) in points]\n",
    "    return math.sqrt(sum(diffs_squared_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) given a training set and a test instance, use getDistance to calculate all pairwise distances\n",
    "def get_neighbours(training_set, test_instance, k):\n",
    "    distances = [_get_tuple_distance(training_instance, test_instance) for training_instance in training_set]\n",
    "    # index 1 is the calculated distance between training_instance and test_instance\n",
    "    sorted_distances = sorted(distances, key=itemgetter(1))\n",
    "    # extract only training instances\n",
    "    sorted_training_instances = [tuple[0] for tuple in sorted_distances]\n",
    "    # select first k elements\n",
    "    return sorted_training_instances[:k]\n",
    " \n",
    "def _get_tuple_distance(training_instance, test_instance):\n",
    "    return (training_instance, get_distance(test_instance, training_instance[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) given an array of nearest neighbours for a test case, tally up their classes to vote on test case class\n",
    "def get_majority_vote(neighbours):\n",
    "    # index 1 is the class\n",
    "    classes = [neighbour[1] for neighbour in neighbours]\n",
    "    count = Counter(classes)\n",
    "    return count.most_common()[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x0000000007DDD5C8>\n",
      "Classifying test instance number 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f5ebb3935866>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classifying test instance number '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mneighbours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_neighbours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mmajority_vote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_majority_vote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneighbours\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# load the data and create the training and test sets\n",
    "# random_state = 1 is just a seed to permit reproducibility of the train/test split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=1)\n",
    "\n",
    "# reformat train/test datasets for convenience\n",
    "train = np.array(zip(X_train,y_train))\n",
    "test = np.array(zip(X_test, y_test))\n",
    "\n",
    "# generate predictions\n",
    "predictions = []\n",
    "\n",
    "# We are skipping k optimization due to shortage of time\n",
    "# let's arbitrarily set k equal to 5, meaning that to predict the class of new instances,\n",
    "k = 5\n",
    "print(test)\n",
    "# for each instance in the test set, get nearest neighbours and majority vote on predicted class\n",
    "for x in range(len(X_test)):\n",
    "        print('Classifying test instance number ' + str(x))\n",
    "        print(test[x])\n",
    "        neighbours = get_neighbours(training_set=train, test_instance=test[x][0], k=5)\n",
    "        majority_vote = get_majority_vote(neighbours)\n",
    "        predictions.append(majority_vote)\n",
    "        print('Predicted label=' + str(majority_vote) + ', Actual label=' + str(test[x][1]))\n",
    "\n",
    "# summarize performance of the classification\n",
    "print('\\nThe overall accuracy of the model is: ' + str(accuracy_score(y_test, predictions)) + \"\\n\")\n",
    "report = classification_report(y_test, predictions, target_names = iris.target_names)\n",
    "print('A detailed classification report: \\n\\n' + report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dog-project]",
   "language": "python",
   "name": "conda-env-dog-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
